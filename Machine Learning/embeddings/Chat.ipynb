{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beba44-9168-4248-b14d-b2fe3a6b4650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037e6c5-d3ea-492d-a21f-0fc2bd0a8236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"codeparrot/codeparrot-small\")\n",
    "outputs = pipe(\"flask route to return pi\", max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6748b-4160-46f0-aeca-f19bb4c3248c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e3b16-8b2f-456e-9b70-d016d6cc75e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6528159-7b1e-42b3-a100-24fac139ff15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5446b4-f8a2-4efd-857b-7f293d48a8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b30f07ee28417ab8bbf4ad80f6cd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431abac8c2fb4eb2820a2832346b424f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df959ddcebab49bc915699b9480ce369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2c9b78d9d74bc990efc3a2243e6f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426237d8a4f247e0b9ac837bea59d262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2749052e0f4a4796929a8722912117f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable version of something with a deep, powerful and expressive\"},\n",
       " {'generated_text': \"Hello, I'm a language model, this is my first commit and I'm going to put it on github. I've also put my code in\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and you'll hear me in this chapter!\\n\\nOne of the features we love is the ability to have\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a developer. If you don't know my code, please don't comment anything on the repo, because\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I'm not a language designer. That should probably be a rule not a rule. I am speaking as a\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db87ae-40a7-4fbc-a3aa-4cc8db42dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcd7b5-fb05-4a14-bb26-d8bf9b321073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9719c03-c2c2-4014-b8e7-f94d4390892b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582bbc1-2ee7-4926-9beb-94414faddce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9081fbf-2da2-4f1f-84a0-ca20d94b8ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT3ForCausalLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizerFast, GenerationConfig\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT3ForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuiHuang/gpt3-damo-large-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gpt3'"
     ]
    }
   ],
   "source": [
    "from gpt3 import GPT3ForCausalLM\n",
    "from transformers import BertTokenizerFast, GenerationConfig\n",
    "\n",
    "\n",
    "model = GPT3ForCausalLM.from_pretrained(\"HuiHuang/gpt3-damo-large-zh\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"HuiHuang/gpt3-damo-large-zh\")\n",
    "generate_config = GenerationConfig(\n",
    "    max_new_tokens=121, eos_token_id=tokenizer.sep_token_id,\n",
    "    no_repeat_ngram_size=3, top_p=0.9, do_sample=True)\n",
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    user = input(\">>>\")\n",
    "    model_inputs = tokenizer(\n",
    "        user, padding=False, add_special_tokens=False, return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "    attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "    generated_sequence = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        generation_config=generate_config\n",
    "    )\n",
    "    generated_sequence = generated_sequence[0]\n",
    "\n",
    "    pred = tokenizer.decode(\n",
    "        generated_sequence,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True)\n",
    "    pred = pred.replace(\" \", \"\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7329b-c298-4afd-acb4-e7bef794c40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
