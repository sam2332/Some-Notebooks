{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ab478a-8963-4f4d-a211-ab05430ef7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Libs.FunctionBot import GPTtoolCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_caller = GPTtoolCaller(system_msg=\"\"\"\n",
    "Please take the users input.\n",
    "TAlk to yourself consicely between tool calls. this is your memory.\n",
    "do not write messages to the user, they will never see it.\n",
    "use only the tools you have been given\n",
    "There are memory functions: mem_recall, mem_memorize, mem_search, mem_forget\n",
    "\n",
    "if you cannot answer then please call the tool UNKNOWN_command\n",
    "\"\"\")\n",
    "\n",
    "import tempfile\n",
    "\n",
    "from Command_Library.Memory import setup as memory_setup\n",
    "memory_setup(gpt_caller)\n",
    "\n",
    "from Command_Library.FileIO import setup as fileio_setup\n",
    "fileio_setup(gpt_caller)\n",
    "\n",
    "from Command_Library.ShellCmd import setup as shellcmd_setup\n",
    "shellcmd_setup(gpt_caller)\n",
    "\n",
    "@gpt_caller.register_tool\n",
    "def UNKNOWN_command(command):\n",
    "    \"\"\"This tool is called when the system does not know how to handle a command\"\"\"\n",
    "    print(f\"UNKNOWN COMMAND: {command}\")\n",
    "    return f\"UNKNOWN_command: {command}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_chat():\n",
    "    print(\"Welcome to the GPT-4 chatbot, type 'exit' to quit\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "        print(\"User:\",user_input)\n",
    "        gpt_caller.query(user_input)\n",
    "run_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_caller.reset()\n",
    "messages = gpt_caller.query(\"\"\"\n",
    "read the file at fake_bank_with_dummy_account.py and modify it to be a multi-account fast-api server service\n",
    "after that: modify the homepage to have a list of accounts and a link to each account, and a link to create a new account\n",
    "after that: modify the account page to have a list of transactions and a link to create a new transaction\n",
    "make sure to add routes to the api to support these new features\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_caller.reset()\n",
    "messages = gpt_caller.query(\"\"\" review code quality for fake_bank_with_dummy_account.py \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_caller.reset()\n",
    "messages = gpt_caller.query(\"\"\" review code quality for bad_code.py \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (782055412.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[168], line 28\u001b[0;36m\u001b[0m\n\u001b[0;31m    685     )\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "gpt_caller.reset()---------------------------------------------------------------------------\n",
    "InvalidRequestError                       Traceback (most recent call last)\n",
    "/home/srudloff/AI/apiplay/Final Function Caller.ipynb Cell 7 line 2\n",
    "      1 gpt_caller.reset()\n",
    "----> 2 messages = gpt_caller.query(\"\"\" rewrite the code inside  bad_code.py and save as better.py \"\"\")\n",
    "\n",
    "/home/srudloff/AI/apiplay/Final Function Caller.ipynb Cell 7 line 7\n",
    "     68 stopped = False\n",
    "     69 while not stopped:\n",
    "---> 70     response = openai.ChatCompletion.create(\n",
    "     71         model=self.model_name,\n",
    "     72         messages=self.messages,            \n",
    "     73         tools=self.tool_definitions,\n",
    "     74         tool_choice=\"auto\",\n",
    "     75         \n",
    "     76     )\n",
    "     78     print(response['choices'])\n",
    "     79     for choice in response['choices']:\n",
    "     80         #print(choice)\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25, in ChatCompletion.create(cls, *args, **kwargs)\n",
    "     23 while True:\n",
    "     24     try:\n",
    "---> 25         return super().create(*args, **kwargs)\n",
    "     26     except TryAgain as e:\n",
    "...\n",
    "    684         rbody, rcode, resp.data, rheaders, stream_error=stream_error\n",
    "    685     )\n",
    "    686 return resp\n",
    "\n",
    "InvalidRequestError: Invalid value for 'content': expected a string, got null.\n",
    "messages = gpt_caller.query(\"\"\" rewrite the code inside  bad_code.py and save as better.py \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x7f57106862a0> JSON: {\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"content\": null,\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"function\": {\n",
      "          \"arguments\": \"{\\\"path\\\":\\\"chatgui.py\\\"}\",\n",
      "          \"name\": \"read_file\"\n",
      "        },\n",
      "        \"id\": \"call_WFFCOZ4K31zlJAOEXhvJa0re\",\n",
      "        \"type\": \"function\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}]\n",
      "read_file {\"path\":\"chatgui.py\"}\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4be71283e5635404dcdb50c361937441 in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4be71283e5635404dcdb50c361937441 in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4be71283e5635404dcdb50c361937441 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Thu, 07 Dec 2023 19:41:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-1106', 'openai-organization': 'user-iurrhgfo1igowph1bcqi1ybd', 'openai-processing-ms': '2597', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '60000', 'x-ratelimit-limit-tokens_usage_based': '60000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '58601', 'x-ratelimit-remaining-tokens_usage_based': '58196', 'x-ratelimit-reset-requests': '16.397s', 'x-ratelimit-reset-tokens': '1.399s', 'x-ratelimit-reset-tokens_usage_based': '1.803s', 'x-request-id': '4be71283e5635404dcdb50c361937441', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '831f3edae980616a-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/srudloff/AI/apiplay/Final Function Caller.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gpt_caller\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m messages \u001b[39m=\u001b[39m gpt_caller\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\"\"\u001b[39;49m\u001b[39m \u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mmodify chatgui.py wo be able to be fully resized\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/srudloff/AI/apiplay/Final Function Caller.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m stopped \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m stopped:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages,            \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         tools\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtool_definitions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m         tool_choice\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srudloff/AI/apiplay/Final%20Function%20Caller.ipynb#X10sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         \u001b[39m#print(choice)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4be71283e5635404dcdb50c361937441 in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4be71283e5635404dcdb50c361937441 in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4be71283e5635404dcdb50c361937441 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Thu, 07 Dec 2023 19:41:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-1106', 'openai-organization': 'user-iurrhgfo1igowph1bcqi1ybd', 'openai-processing-ms': '2597', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '60000', 'x-ratelimit-limit-tokens_usage_based': '60000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '58601', 'x-ratelimit-remaining-tokens_usage_based': '58196', 'x-ratelimit-reset-requests': '16.397s', 'x-ratelimit-reset-tokens': '1.399s', 'x-ratelimit-reset-tokens_usage_based': '1.803s', 'x-request-id': '4be71283e5635404dcdb50c361937441', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '831f3edae980616a-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}"
     ]
    }
   ],
   "source": [
    "gpt_caller.reset()\n",
    "messages = gpt_caller.query(\"\"\" \n",
    "modify chatgui.py wo be able to be fully resized\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x7fd99cfa27a0> JSON: {\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"content\": null,\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"function\": {\n",
      "          \"arguments\": \"{\\\"path\\\":\\\"chatgui.py\\\"}\",\n",
      "          \"name\": \"read_file\"\n",
      "        },\n",
      "        \"id\": \"call_Hm5WMfSmF46vwGFr1O0A1NVg\",\n",
      "        \"type\": \"function\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}]\n",
      "read_file {\"path\":\"chatgui.py\"}\n",
      "[<OpenAIObject at 0x7fd99cfa20c0> JSON: {\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"content\": null,\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"function\": {\n",
      "          \"arguments\": \"{\\\"path\\\":\\\"chatgui.py\\\",\\\"content\\\":\\\"\\\\n        with open('chat_log.txt', 'a') as f:\\\\n            f.write(f'User: {user_input}\\\\\\\\n')\\\\n\\\"}\",\n",
      "          \"name\": \"append_file\"\n",
      "        },\n",
      "        \"id\": \"call_7nhtVWkSYhQVH7OqFNTfka5V\",\n",
      "        \"type\": \"function\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}]\n",
      "append_file {\"path\":\"chatgui.py\",\"content\":\"\\n        with open('chat_log.txt', 'a') as f:\\n            f.write(f'User: {user_input}\\\\n')\\n\"}\n",
      "[<OpenAIObject at 0x7fd99cfa3bf0> JSON: {\n",
      "  \"finish_reason\": \"stop\",\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"content\": \"The file has been updated to save each message the user types.\",\n",
      "    \"role\": \"assistant\"\n",
      "  }\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "gpt_caller.reset()\n",
    "gpt_caller.query(\"read chatgui.py  and please make it save each time the user types a message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
