{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03a3ef3-d285-4d99-bcf6-a949cc80cc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bbc53895c640c8bdf82dbf4d8196eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 442, in load_state_dict\n",
      "    return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 809, in load\n",
      "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1172, in _load\n",
      "    result = unpickler.load()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1142, in persistent_load\n",
      "    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1112, in load_tensor\n",
      "    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16777216 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 446, in load_state_dict\n",
      "    if f.read(7) == \"version\":\n",
      "       ^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1879: character maps to <undefined>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\srudloff\\AppData\\Local\\Temp\\ipykernel_11428\\2306176605.py\", line 4, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\"model\")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 2795, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3109, in _load_pretrained_model\n",
      "    state_dict = load_state_dict(shard_file)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 458, in load_state_dict\n",
      "    raise OSError(\n",
      "OSError: Unable to load weights from pytorch checkpoint file for 'model\\pytorch_model-00001-of-00002.bin' at 'model\\pytorch_model-00001-of-00002.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1022, in get_records\n",
      "    lines, first = inspect.getsourcelines(etb.tb_frame)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\inspect.py\", line 1244, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\inspect.py\", line 1075, in findsource\n",
      "    module = getmodule(object, file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\inspect.py\", line 1000, in getmodule\n",
      "    modulesbyfile[f] = modulesbyfile[\n",
      "    ~~~~~~~~~~~~~^^^\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897eb78-3906-4821-9df0-da1fce16d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half().cuda()\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50278, 50279, 50277, 1, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>What's your mood today?<|ASSISTANT|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=64,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    ")\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12f3d7-2264-47d7-b8d3-2a3c25d3fcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
