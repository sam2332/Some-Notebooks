{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa204b1-96a4-4158-b6e3-d9ee87506e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from datetime import datetime\n",
    "import keyring\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4e5ab6-9864-4a78-80c8-ab7544daac6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jinja2\n",
    "from jinja2 import Template\n",
    "\n",
    "def _TS(TEMPLATE_INPUT, args={}):\n",
    "    # Always use FileSystemLoader with the current directory\n",
    "    templateLoader = jinja2.FileSystemLoader(searchpath=\"./\")\n",
    "    templateEnv = jinja2.Environment(loader=templateLoader)\n",
    "    \n",
    "    # Check if the input is a file in the current directory\n",
    "    if os.path.isfile(TEMPLATE_INPUT):\n",
    "        # Load from a file\n",
    "        template = templateEnv.get_template(TEMPLATE_INPUT)\n",
    "    else:\n",
    "        # If not a file, load from a string\n",
    "        # However, the FileSystemLoader is still in effect for any imported/included templates\n",
    "        template = Template(TEMPLATE_INPUT)\n",
    "        template.environment = templateEnv\n",
    "\n",
    "    return template.render(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d941d27a-da00-45bc-b298-6118e98a4932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_key =keyring.get_password(\"system\", \"openai_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e497e70-c216-49c6-a965-7292c92fbe5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChatRoom:\n",
    "    def __init__(self, room_id=None,model='gpt-3.5-turbo',system_prompt=\"You are a helpful assistant.\",save=True):\n",
    "        if room_id is None:\n",
    "            room_id = uuid.uuid4()\n",
    "        self.room_id = room_id\n",
    "        self.chat_history_file = f'{room_id}_chat_history.json'\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "        self.save = save\n",
    "        \n",
    "        self.chat_history = []\n",
    "        if save:\n",
    "            if not os.path.exists(self.chat_history_file):\n",
    "                with open(self.chat_history_file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "            else:\n",
    "                self.get_chat_history()\n",
    "            \n",
    "    def get_chat_history(self):\n",
    "        if self.save:\n",
    "            with open(self.chat_history_file, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "\n",
    "    def save_chat_history(self):\n",
    "        if self.save:\n",
    "            with open(self.chat_history_file, 'w') as f:\n",
    "                json.dump(self.chat_history, f)\n",
    "            \n",
    "    def clone(self):\n",
    "        new_room_id = f\"{self.room_id}_{uuid.uuid4()}\"\n",
    "        new_chatroom = ChatRoom(new_room_id, model=self.model, system_prompt=self.system_prompt,save=self.save)\n",
    "        \n",
    "        # Copy chat history\n",
    "        new_chatroom.chat_history = self.chat_history\n",
    "        new_chatroom.save_chat_history()\n",
    "        \n",
    "        return new_chatroom\n",
    "    \n",
    "    def send_message(self, message,respond = True,max_tokens=2000,temperature=0.7):\n",
    "        self.chat_history.append({\n",
    "            'content': message,\n",
    "            'role': 'user'\n",
    "        })\n",
    "\n",
    "        if respond:\n",
    "            # Prepare the messages for the API call.\n",
    "            api_messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "            # Accurately estimate the token count using Tiktoken and trim messages accordingly.\n",
    "            token_count = 0\n",
    "            for msg in reversed(self.chat_history):\n",
    "                api_messages.insert(1,msg)  # Insert each message at the start to keep the order.\n",
    "\n",
    "            self.last_response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=api_messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens \n",
    "            ).choices[0].message['content']\n",
    "\n",
    "            self.chat_history.append({\n",
    "                'content': self.last_response,\n",
    "                'role': 'assistant'\n",
    "            })\n",
    "        self.save_chat_history()\n",
    "\n",
    "        if respond:\n",
    "            return self.last_response\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81acc7e-5ee3-4099-b904-b8b88e3014f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess a file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--file_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt.AiPrompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to the file to process\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Process a file.\")\n",
    "parser.add_argument('--file_path', type=str, default=\"Prompt.AiPrompt\", help=\"Path to the file to process\")\n",
    "args = parser.parse_args()\n",
    "file_path = Path(args.file_path)\n",
    "if file_path.is_file():\n",
    "    FILE =_TS(Path(\".AiPrompt\").read_text(),locals())\n",
    "    chat_room = ChatRoom(system_prompt=\"\"\"\n",
    "    ## RESPONSE INSTRUCTION:\n",
    "        Only answer what is asked. Do not write pre-text,post-text,disclaimers or \n",
    "        explanations about your limitations or the ethical merits of any part of the conversation.\n",
    "        Do not talk about yourself. Don't introduce unnecessary fluff into answers.\n",
    "        Always answer what is asked. If you cannot answer, only reply that you cannot answer and do not elaborate.\n",
    "        Avoid including patronizing or pedentic elaboration, explanation or advice that hasn't been asked for.\n",
    "        Always follow these guidelines.\n",
    "\n",
    "    Please processs this input file and then follow the directions at the end of the document!\n",
    "    FILE INPUT WILL BE NEXT MESSAGE\n",
    "    \"\"\".strip(),save=False,model='gpt-4')\n",
    "    output  = chat_room.send_message(FILE)\n",
    "    outfile = Path(\"Prompt.AiPrompt.lastOutput\")\n",
    "    print(output)\n",
    "    buffer = []\n",
    "    if outfile.exists():\n",
    "        buffer.extend(outfile.read_text().splitlines())\n",
    "    print(\"Send blank line to exit chat\")\n",
    "    uin = None\n",
    "    try:\n",
    "        while uin != \"\":\n",
    "            if uin is not None:\n",
    "                buffer.append(f\"# {uin}\")\n",
    "                output  = chat_room.send_message(uin)\n",
    "                buffer.append(f\"{output}\")\n",
    "                print(output)\n",
    "            uin = input(\":>\").strip()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    outfile.write_text(\"\\n\".join(buffer))\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"No such file: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de77eaa-3c9e-42b6-a021-d65df9aba728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab7cab-5cbf-4cd6-91fd-e96ffc53010a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
